"""
This demo script shows how to send audio data to Audio2Face Streaming Audio Player via gRPC requests.
There are two options:
 * Send the whole track at once using PushAudioRequest()
 * Send the audio chunks seuqntially in a stream using PushAudioStreamRequest()
For the second option this script emulates the stream of chunks, generated by splitting an input WAV audio file.
But in a real application such stream of chunks may be aquired from some other streaming source:
 * streaming audio via internet, streaming Text-To-Speech, etc
gRPC protocol details could be find in audio2face.proto
"""

import sys
import time
import keyboard
import audio2face_pb2
import audio2face_pb2_grpc
import grpc
import numpy as np
import soundfile
import speech_recognition as sr
from gtts import gTTS
import io
from scipy.io import wavfile
from pydub import AudioSegment
import wave


def get_tts_data(text: str) -> bytes:
    """
    Generate Text-to-Speech (TTS) audio in mp3 format.
    
    Parameters:
        text (str): The text to be converted to speech.
        
    Returns:
        bytes: TTS audio in mp3 format.
    """
    # Create a BytesIO object to hold the TTS audio data in mp3 format
    tts_result = io.BytesIO()
    # Generate TTS audio using gTTS library with the specified text and language (en-US)
    tts = gTTS(text=text, lang='en-US', slow=True)
    # Write the TTS audio data to the BytesIO object
    tts.write_to_fp(tts_result)
    tts_result.seek(0)
    with open("result.wav", 'wb') as f:
        f.write(tts_result.getvalue())
    # Read and return the TTS audio data as bytes
    return tts_result.read()

def tts_to_wav(tts_byte: bytes, framerate: int = 44100) -> np.ndarray:
    """
    Convert TTS audio from mp3 format to WAV format and set the desired frame rate and channels.
    
    Parameters:
        tts_byte (bytes): TTS audio in mp3 format.
        framerate (int, optional): Desired frame rate for the WAV audio. Defaults to 22050.
        
    Returns:
        numpy.ndarray: TTS audio in WAV format as a numpy array of float32 values.
    """
   # Convert the TTS audio bytes in mp3 format to a pydub AudioSegment object
    seg = AudioSegment.from_mp3(io.BytesIO(tts_byte))
    # Set the frame rate and number of channels for the audio
    seg = seg.set_frame_rate(framerate)
    seg = seg.set_channels(1)
    # Create a BytesIO object to hold the WAV audio data
    wavIO = io.BytesIO()
    # Export the AudioSegment as WAV audio to the BytesIO object
    seg.export(wavIO, format="wav")
    wavIO.seek(0)
    # Read the WAV audio data from the BytesIO object using scipy.io.wavfile.read()
    rate, wav = read(wavIO)
    return wav


def save_wav_from_bytes(audio_bytes, filename, framerate=44100):
    with wave.open(filename, 'wb') as wav_file:
        # Set the WAV file parameters
        wav_file.setnchannels(1)  # Mono
        wav_file.setsampwidth(2)  # 2 bytes (16 bits) per sample
        wav_file.setframerate(framerate)  # Sample rate

        # Write the audio data to the WAV file
        wav_file.writeframes(audio_bytes)


def push_audio_track(url, audio_data, samplerate, instance_name):
    """
    This function pushes the whole audio track at once via PushAudioRequest()
    PushAudioRequest parameters:
     * audio_data: bytes, containing audio data for the whole track, where each sample is encoded as 4 bytes (float32)
     * samplerate: sampling rate for the audio data
     * instance_name: prim path of the Audio2Face Streaming Audio Player on the stage, were to push the audio data
     * block_until_playback_is_finished: if True, the gRPC request will be blocked until the playback of the pushed track is finished
    The request is passed to PushAudio()
    """

    block_until_playback_is_finished = True  # ADJUST
    with grpc.insecure_channel(url) as channel:
        stub = audio2face_pb2_grpc.Audio2FaceStub(channel)
        request = audio2face_pb2.PushAudioRequest()
        request.audio_data = audio_data.astype(np.float32).tobytes()
        request.samplerate = samplerate
        request.instance_name = instance_name
        request.block_until_playback_is_finished = block_until_playback_is_finished
        print("Sending audio data...")
        response = stub.PushAudio(request)
        if response.success:
            print("SUCCESS")
        else:
            print(f"ERROR: {response.message}")
    print("Closed channel")


def push_audio_track_stream(url, audio_data, samplerate, instance_name):
    """
    This function pushes audio chunks sequentially via PushAudioStreamRequest()
    The function emulates the stream of chunks, generated by splitting input audio track.
    But in a real application such stream of chunks may be aquired from some other streaming source.
    The first message must contain start_marker field, containing only meta information (without audio data):
     * samplerate: sampling rate for the audio data
     * instance_name: prim path of the Audio2Face Streaming Audio Player on the stage, were to push the audio data
     * block_until_playback_is_finished: if True, the gRPC request will be blocked until the playback of the pushed track is finished (after the last message)
    Second and other messages must contain audio_data field:
     * audio_data: bytes, containing audio data for an audio chunk, where each sample is encoded as 4 bytes (float32)
    All messages are packed into a Python generator and passed to PushAudioStream()
    """

    chunk_size = samplerate // 10  # ADJUST
    sleep_between_chunks = 0.04  # ADJUST
    block_until_playback_is_finished = True  # ADJUST

    with grpc.insecure_channel(url) as channel:
        print("Channel created")
        stub = audio2face_pb2_grpc.Audio2FaceStub(channel)

        def make_generator():
            start_marker = audio2face_pb2.PushAudioRequestStart(
                samplerate=samplerate,
                instance_name=instance_name,
                block_until_playback_is_finished=block_until_playback_is_finished,
            )
            # At first, we send a message with start_marker
            yield audio2face_pb2.PushAudioStreamRequest(start_marker=start_marker)
            # Then we send messages with audio_data
            for i in range(len(audio_data) // chunk_size + 1):
                time.sleep(sleep_between_chunks)
                chunk = audio_data[i * chunk_size : i * chunk_size + chunk_size]
                yield audio2face_pb2.PushAudioStreamRequest(audio_data=chunk.astype(np.float32).tobytes())

       
        request_generator = make_generator()
        print("Sending audio data...")
        response = stub.PushAudioStream(request_generator)
        if response.success:
            print("SUCCESS")
        else:
            print(f"ERROR: {response.message}")
    print("Channel closed")

def ask_name():
    
def main():
    """
    This demo script shows how to send audio data to Audio2Face Streaming Audio Player via gRPC requests.
    There two options:
     * Send the whole track at once using PushAudioRequest()
     * Send the audio chunks seuqntially in a stream using PushAudioStreamRequest()
    For the second option this script emulates the stream of chunks, generated by splitting an input WAV audio file.
    But in a real application such stream of chunks may be aquired from some other streaming source:
     * streaming audio via internet, streaming Text-To-Speech, etc
    gRPC protocol details could be find in audio2face.proto
    """
    keyboard.
    while(True):
        keyboard.wait('esc')
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            print("Listening...")
            audio = recognizer.listen(source)
        text = ""
        try:
            print("Recognizing...")
            text = recognizer.recognize_google(audio)
            print("You said:", text)
        except sr.UnknownValueError:
            text = "Sorry, I couldn't understand what you said."
        except sr.RequestError as e:
            text = "Sorry, I couldn't understand what you said."
        
        
        get_tts_data(text)
        # save_wav_from_bytes(get_tts_data(text), "result.wav")
        # Sleep time emulates long latency of the request
        sleep_time = 0  # ADJUST

        # URL of the Audio2Face Streaming Audio Player server (where A2F App is running)
        url = "localhost:50051"  # ADJUST

        # Local input WAV file path
        audio_fpath = "result.wav"

        # Prim path of the Audio2Face Streaming Audio Player on the stage (were to push the audio data)
        instance_name = "/World/audio2face/PlayerStreaming_02"

        data, samplerate = soundfile.read(audio_fpath, dtype="float64")
        print(len(data))
        print(data)
        print(samplerate)
        # Only Mono audio is supported
        if len(data.shape) > 1:
            data = np.average(data, axis=1)

        print(f"Sleeping for {sleep_time} seconds")
        time.sleep(sleep_time)
   
        push_audio_track_stream(url, data, samplerate, instance_name)
        # else:
        #     continue
        # push_audio_track_stream(url, data, samplerate, instance_name)
        # if 0:  # ADJUST
        #     # Push the whole audio track at once
       
        # else:
        #     # Emulate audio stream and push audio chunks sequentially
           


if __name__ == "__main__":
    main()
